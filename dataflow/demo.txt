- Dump schema from current table
      In my example that's BigQuery... strip down to columsn I wants
      bq show --schema --format prettyjson bigquery-public-data:austin_311.311_request > schema.json
      drop columns, adjust descriptions, names, and types

- If couldn't dump to file, create your json schema file with the schema specs
      https://cloud.google.com/bigquery/docs/schemas#specifying_a_json_schema_file


- Run from gcloud 
gcloud dataflow jobs run 20201109-003 \
--gcs-location gs://dataflow-templates/latest/GCS_Text_to_BigQuery \
--region=us-central1 --network=custom-vpc --subnetwork=regions/us-central1/subnetworks/sn-central1 \
--parameters \
javascriptTextTransformFunctionName=transform,\
JSONPath=gs://ce-demo2/dataflow/311_request_schema.json,\
javascriptTextTransformGcsPath=gs://ce-demo2/dataflow/311_request_udf.js,\
inputFilePattern=gs://ce-demo2/dataflow/311_request.json,\
outputTable=ce-demo2:bq_demo.311_landing,\
bigQueryLoadingTemporaryDirectory=gs://ce-demo2/dataflow/tmp/


11/9
Adding steps above

11/8/2020
From Dataflow docs CONCEPTS section: What is a Template?
Classic template DAG stored in file in GCS. Flex template stored in container in GCR
Template separate dev from execution.
https://cloud.google.com/dataflow/docs/concepts/dataflow-templates?hl=en_US


11/7/2020
https://cloud.google.com/dataflow/docs/guides/templates/provided-batch?hl=en_US#gcstexttobigquery
#JSON is created in gs://cedemo2/dataflow/schema.JSON
#data file csv is in 




# HYPOTHESES
# Subscription templates will be stateful by pulling (ack) from existing subscriptions
# Topic tepmlates will be stateless because they'll make their own subscription
# as part of runtime. Perhaps better for testing
# The java libraries in github creates JSON configs to pass to Dataflow runner
# The files stored in the versioned buckets are JSON outputs.


# Dataflow Templates in Google Cloud Docs
# https://cloud.google.com/dataflow/docs/guides/templates/provided-streaming#cloudpubsubtobigquery

# Dataflow Java Source in GitHub
#https://github.com/GoogleCloudPlatform/DataflowTemplates/blob/master/src/main/java/com/google/cloud/teleport/templates/PubSubToBigQuery.java

# Full Tutorial in GCP Docs
#https://cloud.google.com/solutions/performing-etl-from-relational-database-into-bigquery

#TODO Output tables in BQ must exist before running pipeline
#TODO pick raspi data topic in pubsub
#TODO Define Input topic and output table below
# inputTopic	The Pub/Sub input topic to read from, in the format of projects/<project>/topics/<topic>.
# outputTableSpec	The BigQuery output table location, in the format of <my-project>:<my-dataset>.<my-table>
#TODO pick latest template, see instructions at #1 gs://dataflow-templates/VERSION/PubSub_to_BigQuery






#Separate Java Maven work

# https://cloud.google.com/dataflow/docs/quickstarts/quickstart-java-maven
# auth command and key in home folder
# see WorkCount.java in this dir for more info

# Change to reflect your Project, Buckets, Network, [Subnet if not default]
mvn -Pdataflow-runner compile exec:java \
      -Dexec.mainClass=org.apache.beam.examples.WordCount \
      -Dexec.args="--project=ce-demo2 \
      --stagingLocation=gs://ce-demo2/dataflow/staging/ \
      --output=gs://ce-demo2/dataflow/output \
      --runner=DataflowRunner \
      --region=us-central1 \
      --network=custom-vpc
      --subnetwork=regions/us-central1/subnetworks/sn-central1"