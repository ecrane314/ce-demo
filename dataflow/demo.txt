# https://cloud.google.com/dataflow/docs/guides/templates/provided-streaming#cloudpubsubtobigquery

#https://github.com/GoogleCloudPlatform/DataflowTemplates/blob/master/src/main/java/com/google/cloud/teleport/templates/PubSubToBigQuery.java
https://cloud.google.com/solutions/performing-etl-from-relational-database-into-bigquery


#TODO Output tables in BQ must exist before running pipeline
#TODO pick raspi data topic in pubsub

#TODO Define Input topic and output table below

#inputTopic	The Pub/Sub input topic to read from, in the format of projects/<project>/topics/<topic>.
#outputTableSpec	The BigQuery output table location, in the format of <my-project>:<my-dataset>.<my-table>

#TODO pick latest template, see instructions at #1 gs://dataflow-templates/VERSION/PubSub_to_BigQuery



# HYPOTHESIS
# Subscription templates will be stateful by pulling (ack) from existing subscriptions
# Topic tepmlates will be stateless because they'll make their own subscription
# as part of runtime. Perhaps better for testing

# https://cloud.google.com/dataflow/docs/quickstarts/quickstart-java-maven
# auth command and key in home folder
# see WorkCount.java in this dir for more info

# Change to reflect your Project, Buckets, Network, [Subnet if not default]
mvn -Pdataflow-runner compile exec:java \
      -Dexec.mainClass=org.apache.beam.examples.WordCount \
      -Dexec.args="--project=ce-demo2 \
      --stagingLocation=gs://ce-demo2/dataflow/staging/ \
      --output=gs://ce-demo2/dataflow/output \
      --runner=DataflowRunner \
      --region=us-central1 \
      --network=custom-vpc
      --subnetwork=regions/us-central1/subnetworks/sn-central1"